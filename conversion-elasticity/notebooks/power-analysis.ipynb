{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec42561b",
   "metadata": {},
   "source": [
    "# Power analysis for conversion\n",
    "\n",
    "**Hypothesis**: raising (lowering) the recommended booked budget will lower (raise) conversion.\n",
    "\n",
    "If this is true, we can change the default recommendation to maximise revenue\n",
    "\n",
    "```math\n",
    "\\textup{Revenue}(\\textup{budget recommendation}) = \\textup{conversion}(\\textup{budget recommendation}) \\times \\textup{budget recommendation}\n",
    "```\n",
    "\n",
    "And we can choose the recommendation that maximises $\\textup{DH Revenue}$. This is not our long term goal, which is vendor value. But learning how to measure response to our policies will be necessary to optimise for the more complicated problem of vendor value.\n",
    "\n",
    "We want to estimate the $\\textup{conversion}$ mapping from budget recommendation size to conversion. \n",
    "\n",
    "We can do this by randomly varying the budget recommendation and observing conversion rates.\n",
    "\n",
    "**Findings**\n",
    "\n",
    "1. For a significant difference in a month long basic A/B test, we could detect 10% relative uplift using about 1/3 vendors.\n",
    "2. Our key parameter is the _elasticity_, and we want to test that this is signficantly different from one.\n",
    "\n",
    "*Note*\n",
    "- We can also do this for retention and vendor outcomes. These are both likely to be harder to measure, because they are noisier. First, retention and vendor outcomes are both conditional on self service bookings. The self service booking rate is low, [around 1.5% over 2025](https://tableau.deliveryhero.net/#/site/GlobalStandardReporting/views/SelfBookingAnalytics/MBRproduct?=null&:iid=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22045da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from statsmodels.stats.proportion import proportion_effectsize\n",
    "from statsmodels.stats.power import zt_ind_solve_power\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4ee6fc",
   "metadata": {},
   "source": [
    "- Based on the [Self Booking dashboard](https://tableau.deliveryhero.net/#/site/GlobalStandardReporting/views/SelfBookingAnalytics/MBRproduct?=null&:iid=1), we have a Vendor Portal Conversion Rate around 1.5%.\n",
    "- We use standard power of 0.8 and type two error rate bound of 0.05\n",
    "- We have around 650k users per month\n",
    "\n",
    "Let's find the minimum effect size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eadaf4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHLY_USERS = 650000\n",
    "ALPHA=0.05  # type I error rate\n",
    "BETA=0.2    # type II error rate\n",
    "BASELINE_CVR = 0.015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0685acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size per group: 422,412\n",
      "This is 1.30 times our monthly user base\n"
     ]
    }
   ],
   "source": [
    "RELATIVE_EFFECT = 0.05\n",
    "\n",
    "effect_size = proportion_effectsize(BASELINE_CVR, BASELINE_CVR * (1 + RELATIVE_EFFECT))\n",
    "\n",
    "sample_size = zt_ind_solve_power(\n",
    "    effect_size=effect_size,\n",
    "    power=1-BETA,\n",
    "    alpha=ALPHA,\n",
    "    alternative='two-sided' \n",
    ")\n",
    "\n",
    "print(f\"Sample size per group: {sample_size:,.0f}\")\n",
    "print(f\"This is {2*sample_size/MONTHLY_USERS:.2f} times our monthly user base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9753f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0165\n",
      "Sample size per group: 108,093\n",
      "This is 0.33 times our monthly user base\n"
     ]
    }
   ],
   "source": [
    "RELATIVE_EFFECT = 0.1\n",
    "print(BASELINE_CVR * (1 + RELATIVE_EFFECT))\n",
    "\n",
    "effect_size = proportion_effectsize(BASELINE_CVR, BASELINE_CVR * (1 + RELATIVE_EFFECT))\n",
    "\n",
    "sample_size = zt_ind_solve_power(\n",
    "    effect_size=effect_size,\n",
    "    power=1-BETA,\n",
    "    alpha=ALPHA,\n",
    "    alternative='two-sided' \n",
    ")\n",
    "\n",
    "print(f\"Sample size per group: {sample_size:,.0f}\")\n",
    "print(f\"This is {2*sample_size/MONTHLY_USERS:.2f} times our monthly user base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562c5420",
   "metadata": {},
   "source": [
    "# Let's simulate some data and recover parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "defa10e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.discrete.discrete_model import Logit\n",
    "\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbedc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conversion_probability(alpha: float, beta: float, budget: float) -> float:\n",
    "    # conversion probability (make it decreasing in price, arbitrary functional form)\n",
    "    logit = alpha + beta * np.log(budget)\n",
    "    conv_prob = 1 / (1 + np.exp(-logit))\n",
    "    return conv_prob\n",
    "\n",
    "\n",
    "def get_simulation(N: int, budget_change: float, alpha: float, beta: float) -> pd.DataFrame:\n",
    "    assert budget_change < 1, \"budget_change needs to be less than one\"\n",
    "    assert budget_change > 0, \"budget_change needs to be greater than zero\"\n",
    "\n",
    "    # baseline mean budget\n",
    "    baseline_budget_mean = 100\n",
    "    baseline_budgets = rng.poisson(lam=baseline_budget_mean, size=N)\n",
    "    \n",
    "    # randomly assign up, same, down \n",
    "    assignments = np.random.choice(['down', 'same', 'up'], size=N)\n",
    "    budgets = np.where(assignments == 'down', baseline_budgets*(1-budget_change),\n",
    "                       np.where(assignments == 'up', baseline_budgets*(1+budget_change), baseline_budgets))\n",
    "    conversion_probabilities = [\n",
    "        get_conversion_probability(alpha, beta, b) for b in budgets\n",
    "    ]\n",
    "    # conversion draws\n",
    "    conversion = np.random.binomial(1, conversion_probabilities)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"user_id\": np.arange(N),\n",
    "        \"assignment\": assignments,\n",
    "        \"budget\": budgets,\n",
    "        \"conv_prob\": conversion_probabilities,\n",
    "        \"conversion\": conversion\n",
    "    })\n",
    "\n",
    "    df['base_budget'] = df['budget'].copy()\n",
    "    \n",
    "    # Reverse the randomization\n",
    "    df.loc[df['assignment'] == 'up', 'base_budget'] = df.loc[df['assignment'] == 'up', 'budget'] / (1+budget_change)\n",
    "    df.loc[df['assignment'] == 'down', 'base_budget'] = df.loc[df['assignment'] == 'down', 'budget'] / (1-budget_change)\n",
    "    return df\n",
    "\n",
    "def simulate_outcomes(df: pd.DataFrame, alpha: float, elasticity: float, estimated_elasticity: float, sim_change: float) -> Dict[str, float]:\n",
    "\n",
    "    df['base_conversion_prob'] = [get_conversion_probability(alpha, estimated_elasticity, b) for b in df.base_budget.values]\n",
    "    df['base_conversion'] = [np.random.binomial(1, p) for p in df.base_conversion_prob]\n",
    "    df['expected_revenue'] = df['base_conversion_prob'] * df['base_budget']\n",
    "\n",
    "    df['sim_budget'] = df.base_budget * (1 + sim_change)\n",
    "    df['sim_conversion_prob'] = [get_conversion_probability(alpha, estimated_elasticity, b) for b in df.sim_budget.values]\n",
    "\n",
    "    df['sim_conversion'] = [np.random.binomial(1, p) for p in df.sim_conversion_prob]\n",
    "    df['sim_expected_revenue'] = df['sim_conversion_prob'] * df['sim_budget']\n",
    "\n",
    "    original_exp_revenue = np.sum(df.expected_revenue)\n",
    "    simulated_exp_revenue = np.sum(df.sim_expected_revenue)\n",
    "    return {'original': original_exp_revenue, 'simulated': simulated_exp_revenue}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "edc4bfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True elasticity is -3\n",
      "Baseline conversion rate 0.0258\n"
     ]
    }
   ],
   "source": [
    "alpha = 10\n",
    "elasticity = -3\n",
    "print(f\"True elasticity is {elasticity}\")\n",
    "df = get_simulation(10000, 0.2, alpha, elasticity)\n",
    "print(f\"Baseline conversion rate {df.conversion.mean()}\")\n",
    "\n",
    "results = simulate_outcomes(df, alpha, elasticity, elasticity, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f54d20",
   "metadata": {},
   "source": [
    "First we use a Wald Estimator\n",
    "\n",
    "$$\n",
    "\\theta = \\frac{E[\\ln(\\textup{Conversion}) | \\textup{Higher budget}] - E[\\ln(\\textup{Conversion}) | \\textup{Lower budget}]}\n",
    "{E[\\ln(\\textup{Budget}) | \\textup{Higher Budget}] - E[\\ln\\textup{Budget} | \\textup{Lower Budget}]}\n",
    "$$\n",
    "\n",
    "Note, this is a linear estimator, but our model is non linear. So there will be some approximation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "5d908512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated elasticity (up to same) is -2.46, (-4.20, -0.72)\n",
      "Estimated elasticity (same to down) is -3.41, (-4.56, -2.35)\n",
      "Estimated elasticity (up to down) is -3.01, (-3.73, -2.36)\n"
     ]
    }
   ],
   "source": [
    "def get_wald_estimate_simple(df: pd.DataFrame, Z1_name: str, Z0_name: str) -> float:\n",
    "    # Delta log conversion rate / Delta log budget\n",
    "    p1 = df.loc[df['assignment']==Z1_name]['conversion'].mean()\n",
    "    p0 = df.loc[df['assignment']==Z0_name]['conversion'].mean()\n",
    "    \n",
    "    delta_ln_conversion = np.log(p1) - np.log(p0)\n",
    "    delta_ln_budget = np.mean(np.log(df.loc[df.assignment==Z1_name].budget)) - \\\n",
    "                      np.mean(np.log(df.loc[df.assignment==Z0_name].budget))\n",
    "    \n",
    "    return delta_ln_conversion / delta_ln_budget\n",
    "\n",
    "\n",
    "Z1 = ['up', 'same', 'up']\n",
    "Z0 = ['same', 'down', 'down']\n",
    "for z1, z0 in zip(Z1, Z0):\n",
    "    wald_bs = [get_wald_estimate_simple(df.sample(n=10000, replace=True), z1, z0) for i in range(1000)]\n",
    "    print(f\"Estimated elasticity ({z1} to {z0}) is {np.mean(wald_bs):.2f}, ({np.quantile(wald_bs, 0.05):.2f}, {np.quantile(wald_bs, 0.95):.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000e7314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elasticity: -3\n",
      "IV Logit elasticity: -3.747, ([-6.19969018 -1.29505007])\n",
      "Test of exogeneity (residual coef): 0.618\n",
      "P-value: 0.782\n",
      "Estimated elasticity is -2.99, (-3.59, -2.47)\n"
     ]
    }
   ],
   "source": [
    "def simple_iv_logit(df):\n",
    "    df['log_budget'] = np.log(df['budget'])\n",
    "    \n",
    "    # Create instrument dummies\n",
    "    df['up_dummy'] = (df['assignment'] == 'up').astype(int)\n",
    "    df['down_dummy'] = (df['assignment'] == 'down').astype(int)\n",
    "    \n",
    "    # First stage: log_budget ~ up_dummy + down_dummy\n",
    "    X_first = sm.add_constant(df[['up_dummy', 'down_dummy']])\n",
    "    first_stage = sm.OLS(df['log_budget'], X_first).fit()\n",
    "    df['residual_v1'] = first_stage.resid\n",
    "    \n",
    "    # Second stage logit\n",
    "    X_second = sm.add_constant(df[['log_budget', 'residual_v1']])\n",
    "    logit_model = Logit(df['conversion'], X_second).fit(disp=False)\n",
    "    \n",
    "    elasticity = logit_model.params['log_budget']\n",
    "    ci_elasticity = logit_model.conf_int(alpha=0.05).loc['log_budget'].values\n",
    "    return elasticity, logit_model, ci_elasticity\n",
    "\n",
    "# Usage\n",
    "print(f\"Elasticity: {elasticity}\")\n",
    "est_elasticity, model, ci = simple_iv_logit(df.sample(n=1000, replace=True))\n",
    "print(f\"IV Logit elasticity: {est_elasticity:.3f}, ({model.conf_int(alpha=0.05).loc['log_budget'].values})\")\n",
    "print(f\"Test of exogeneity (residual coef): {model.params['residual_v1']:.3f}\")\n",
    "print(f\"P-value: {model.pvalues['residual_v1']:.3f}\")\n",
    "\n",
    "iv_ests = [(e, True if m.pvalues['residual_v1']> 0.05 else False) for e, m, ci in [simple_iv_logit(df.sample(n=10000, replace=True)) for i in range(100)]]\n",
    "\n",
    "print(f\"Estimated elasticity is {np.mean([x[0] for x in iv_ests]):.2f}, ({np.quantile([x[0] for x in iv_ests], 0.05):.2f}, {np.quantile([x[0] for x in iv_ests], 0.95):.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90712718",
   "metadata": {},
   "source": [
    "## Exploiting estimates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "6b9d0d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original': np.float64(74290.18362859893),\n",
       " 'simulated': np.float64(99188.77750591206)}"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulated_results = simulate_outcomes(df, alpha, elasticity, est_elasticity, -0.1) \n",
    "simulated_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e13b64",
   "metadata": {},
   "source": [
    "By lowering revenue, we can raise revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "1cdfcf05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elasticity: -0.0\n",
      "IV Logit elasticity: -0.188, ([-1.0931741  0.7164683])\n",
      "Test of exogeneity (residual coef): -1.129\n",
      "P-value: 0.203\n",
      "Estimated elasticity is -0.21, (-0.97, 0.50)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'original': np.float64(7635.877650070457),\n",
       " 'simulated': np.float64(8251.144128944452)}"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = -4\n",
    "elasticity = -0.00\n",
    "df = get_simulation(10000, 0.2, alpha, elasticity)\n",
    "print(f\"Elasticity: {elasticity}\")\n",
    "est_elasticity, model, ci = simple_iv_logit(df)\n",
    "print(f\"IV Logit elasticity: {est_elasticity:.3f}, ({model.conf_int(alpha=0.05).loc['log_budget'].values})\")\n",
    "print(f\"Test of exogeneity (residual coef): {model.params['residual_v1']:.3f}\")\n",
    "print(f\"P-value: {model.pvalues['residual_v1']:.3f}\")\n",
    "\n",
    "iv_ests = [(e, True if m.pvalues['residual_v1']> 0.05 else False) for e, m, ci in [simple_iv_logit(df.sample(n=10000, replace=True)) for i in range(100)]]\n",
    "\n",
    "print(f\"Estimated elasticity is {np.mean([x[0] for x in iv_ests]):.2f}, ({np.quantile([x[0] for x in iv_ests], 0.05):.2f}, {np.quantile([x[0] for x in iv_ests], 0.95):.2f})\")\n",
    "\n",
    "\n",
    "simulated_results = simulate_outcomes(df, alpha, elasticity, est_elasticity, 0.1) \n",
    "simulated_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "8c4ceb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True elasticity is -0.1\n",
      "Baseline conversion rate 0.034\n",
      "Raise prices to increase revenue\n"
     ]
    }
   ],
   "source": [
    "alpha = -3\n",
    "elasticity = -1/10\n",
    "print(f\"True elasticity is {elasticity}\")\n",
    "df = get_simulation(10000, 0.2, alpha, elasticity)\n",
    "print(f\"Baseline conversion rate {df.conversion.mean()}\")\n",
    "if abs(elasticity) < 1:\n",
    "    print(\"Raise prices to increase revenue\")\n",
    "else:\n",
    "    print(\"Lower prices to increase revenue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "060f0406",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_params = [\n",
    "    (20, -5),\n",
    "    (15, -4),\n",
    "    (10, -3),\n",
    "    (5, -2),\n",
    "    (0, -1),\n",
    "    (-2, -1/2),\n",
    "    (-3, -1/10)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "888f32d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elasticity is -5\n",
      "Lower prices in 1.00 share of simulations\n",
      "Raise prices in 0.00 share of simulations\n",
      "Elasticity is -4\n",
      "Lower prices in 1.00 share of simulations\n",
      "Raise prices in 0.00 share of simulations\n",
      "Elasticity is -3\n",
      "Lower prices in 1.00 share of simulations\n",
      "Raise prices in 0.00 share of simulations\n",
      "Elasticity is -2\n",
      "Lower prices in 0.52 share of simulations\n",
      "Raise prices in 0.00 share of simulations\n",
      "Elasticity is -1\n",
      "Lower prices in 0.02 share of simulations\n",
      "Raise prices in 0.03 share of simulations\n",
      "Elasticity is -0.5\n",
      "Lower prices in 0.00 share of simulations\n",
      "Raise prices in 0.15 share of simulations\n",
      "Elasticity is -0.1\n",
      "Lower prices in 0.00 share of simulations\n",
      "Raise prices in 0.69 share of simulations\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "elasticity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "uplift",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "586b53b2-8a6b-4cc5-aa57-34ae8071211a",
       "rows": [
        [
         "-5.0",
         "28089.289307967407"
        ],
        [
         "-4.0",
         "15582.050573392757"
        ],
        [
         "-3.0",
         "6768.329844278741"
        ],
        [
         "-2.0",
         "364.24368664052776"
        ],
        [
         "-1.0",
         "2584.9776483092046"
        ],
        [
         "-0.5",
         "4597.322005100355"
        ],
        [
         "-0.1",
         "7221.189765171023"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 7
       }
      },
      "text/plain": [
       "elasticity\n",
       "-5.0    28089.289308\n",
       "-4.0    15582.050573\n",
       "-3.0     6768.329844\n",
       "-2.0      364.243687\n",
       "-1.0     2584.977648\n",
       "-0.5     4597.322005\n",
       "-0.1     7221.189765\n",
       "Name: uplift, dtype: float64"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs_results = []\n",
    "\n",
    "n_sims = 100\n",
    "for alpha, elasticity in gt_params:\n",
    "    print(f\"Elasticity is {elasticity}\")\n",
    "    raise_counter = 0\n",
    "    lower_counter = 0\n",
    "    for i in range(n_sims):\n",
    "        df = get_simulation(10000, 0.2, alpha, elasticity)\n",
    "        iv_est, model, ci = simple_iv_logit(df)\n",
    "        # sanity check\n",
    "        # iv_est = elasticity\n",
    "        # ci = [elasticity, elasticity] \n",
    "        if ci[1] < -1:\n",
    "            # upper bound of estimate is less than -1\n",
    "            # elastic, lower prices\n",
    "            sim_change = -0.1\n",
    "            lower_counter += 1\n",
    "        elif ci[0] > -1:\n",
    "            # lower bound of estimate is greater than -1\n",
    "            # inelastic, raise prices\n",
    "            sim_change = 0.1\n",
    "            raise_counter += 1\n",
    "        else:\n",
    "            # inconclusive\n",
    "            sim_change = 0\n",
    "        simulated_results = simulate_outcomes(df, alpha, elasticity, iv_est, sim_change) \n",
    "        simulated_results['iv_est'] = iv_est\n",
    "        simulated_results['elasticity'] = elasticity\n",
    "        simulated_results['budget_change'] = sim_change\n",
    "\n",
    "        bs_results.append(simulated_results)    \n",
    "\n",
    "    print(f\"Lower prices in {lower_counter / n_sims:.2f} share of simulations\")\n",
    "    print(f\"Raise prices in {raise_counter / n_sims:.2f} share of simulations\")\n",
    "\n",
    "df_bootstrap = pd.DataFrame(bs_results)\n",
    "df_bootstrap['uplift'] = df_bootstrap['simulated'] - df_bootstrap['original']\n",
    "df_bootstrap.groupby(\"elasticity\")['uplift'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff2707d",
   "metadata": {},
   "source": [
    "We can increase revenue in every case, on expectation.\n",
    "\n",
    "Concerns\n",
    "\n",
    "- very hard to get practical estimates for relatively inelastic estimates\n",
    "- with 20% changes up, down we only get significance for around 1/2 estimates when elasticity in {-2, 1/2}\n",
    "- For elasticity < -3, we always decrease, correctly\n",
    "- For elasiticty = -0.1, we only increase 0.7 simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede6fc15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conversion-elasticity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
